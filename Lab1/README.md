## Лабораторная работа 1

Выполнили: <br>
Машина Е. P4241 <br>
Щербаков В. P4216

### Задание

#### Reinforcement Learning (RL)

Reinforcement Learning (Обучение с подкреплением) — это класс машинного обучения, в котором агент обучается принимать решения путем взаимодействия с окружающей средой. Агент принимает действия, получает обратную связь в виде вознаграждения или штрафа, и стремится максимизировать кумулятивное вознаграждение.

#### Q-learning
[Q-learning](https://en.wikipedia.org/wiki/Q-learning) — это один из методов обучения с подкреплением, используемый для обучения агента принимать оптимальные действия в конкретной среде. Агент стремится выучить функцию Q, которая оценивает ожидаемую награду для каждой пары состояние-действие.

#### Тестирование и Валидация

В контексте RL, тестирование и валидация играют важную роль. После обучения модели агента необходимо оценить ее производительность (тестирование) и убедиться в ее способности обобщения к различным ситуациям (валидация).

Тестирование в RL включает в себя запуск обученной модели в реальной среде и измерение ее производительности на основе определенных метрик. Валидация, с другой стороны, может включать в себя проверку способности модели адаптироваться к новым условиям или изменениям в среде.

### Выполнение
[Ссылка на код](https://github.com/mashinakatherina/validation-and-testing-of-AI-systems/blob/trunk/Lab1/RL_%E2%84%961.ipynb)

### Вывод
В рамках самостоятельной работы мы продемонстрировали использование библиотек для работы с окружением OpenAI Gym, обучения модели глубокого обучения на примере задачи MountainCar-v0, и визуализации результатов обучения в Colab с использованием анимации.

1. **Установка библиотек:**
Были установлены необходимые библиотеки и зависимости, такие как Gym, Stable-Baselines3, PyVirtualDisplay, и Xvfb. Это необходимо для работы с окружением, обучения модели и визуализации в Colab.

2. **Импорт и настройка окружения:**
Импортированы библиотеки, такие как Gym и Stable-Baselines3, и создано окружение задачи MountainCar-v0. Пространства наблюдения и действий были извлечены для дальнейшего использования в обучении.

3. **Обучение модели:**
Использован алгоритм DQN для обучения модели глубокого обучения на среде MountainCar-v0. Модель обучалась на протяжении заданного количества временных шагов, и результаты сохранены для дальнейшего использования.

4. **Виртуальный дисплей и визуализация:**
Установлен виртуальный дисплей (Xvfb) для визуализации окружения в Colab. Библиотеки PyVirtualDisplay и Matplotlib использовались для создания анимации, которая отображает визуальные результаты тестирования обученной модели.

5. **Анимация и вывод:**
Создан цикл, в котором обученная модель использовалась для прогнозирования действий в среде. Кадры среды были записаны в виде изображений, а затем использованы для создания анимации. Анимация была отображена в ячейке вывода Colab.

6. **Закрытие среды:**
В конце кода среда была закрыта для освобождения ресурсов.
